{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuyrDzHU31oz",
        "outputId": "26cdb0c5-774a-4961-c778-752255f39a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 Rows of Data:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                5.1               3.5                1.4               0.2   \n",
            "1                4.9               3.0                1.4               0.2   \n",
            "2                4.7               3.2                1.3               0.2   \n",
            "3                4.6               3.1                1.5               0.2   \n",
            "4                5.0               3.6                1.4               0.2   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n",
            "\n",
            "Missing Values:\n",
            "sepal length (cm)    0\n",
            "sepal width (cm)     0\n",
            "petal length (cm)    0\n",
            "petal width (cm)     0\n",
            "target               0\n",
            "dtype: int64\n",
            "\n",
            "Training Features Shape: (120, 4)\n",
            "Test Features Shape: (30, 4)\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Import Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Step 3: View Dataset Info\n",
        "print(\"First 5 Rows of Data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Step 4: Check for Missing Values\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# (Optional) Introduce some missing values for demonstration\n",
        "df.loc[5:10, 'sepal length (cm)'] = np.nan\n",
        "\n",
        "# Step 5: Impute Missing Values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df[['sepal length (cm)']] = imputer.fit_transform(df[['sepal length (cm)']])\n",
        "\n",
        "# Step 6: Encode Target Labels (if not already encoded)\n",
        "# (In this case, already encoded as 0, 1, 2)\n",
        "\n",
        "# Step 7: Feature Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df.drop('target', axis=1))\n",
        "df_scaled = pd.DataFrame(scaled_features, columns=iris.feature_names)\n",
        "df_scaled['target'] = df['target']\n",
        "\n",
        "# Step 8: Split Dataset (80% train, 20% test)\n",
        "X = df_scaled.drop('target', axis=1)\n",
        "y = df_scaled['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Final Output\n",
        "print(\"\\nTraining Features Shape:\", X_train.shape)\n",
        "print(\"Test Features Shape:\", X_test.shape)\n"
      ]
    }
  ]
}